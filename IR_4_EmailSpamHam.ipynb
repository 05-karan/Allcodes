{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mohammed Johar Pathariya\n",
    "# 2347004\n",
    "# <center> Assignment 4 <center>\n",
    "Q. Implement e-mail spam filtering using text classification algorithm with appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EjbO6C99IHL9"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PoM2COHsIms1"
   },
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "\n",
    "df = pd.read_csv('/Users/mohammedpathariya/Docs/BES1/IR/Prac/spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ywXslS_SIuqo",
    "outputId": "fe509646-a38a-4079-df33-69547ac6bc8c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2469</td>\n",
       "      <td>2469</td>\n",
       "      <td>Subject: stock promo mover : cwtd\\n * * * urge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5063</td>\n",
       "      <td>5063</td>\n",
       "      <td>Subject: are you listed in major search engine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12564</td>\n",
       "      <td>12564</td>\n",
       "      <td>Subject: important information thu , 30 jun 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2796</td>\n",
       "      <td>2796</td>\n",
       "      <td>Subject: = ? utf - 8 ? q ? bask your life with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1468</td>\n",
       "      <td>1468</td>\n",
       "      <td>Subject: \" bidstogo \" is places to go , things...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0          2469        2469   \n",
       "1          5063        5063   \n",
       "2         12564       12564   \n",
       "3          2796        2796   \n",
       "4          1468        1468   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Subject: stock promo mover : cwtd\\n * * * urge...      1  \n",
       "1  Subject: are you listed in major search engine...      1  \n",
       "2  Subject: important information thu , 30 jun 20...      1  \n",
       "3  Subject: = ? utf - 8 ? q ? bask your life with...      1  \n",
       "4  Subject: \" bidstogo \" is places to go , things...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hCUm1hD-Jj4Y"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0.1', 'Unnamed: 0']\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0yN2or_Iy6Z",
    "outputId": "1d32bb87-8fb1-45e9-f751-84ec48afa6c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nw6qEdttJ_yS",
    "outputId": "8ef862e8-65f1-4041-f9dc-de6ac7d75e6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: stock promo mover : cwtd\\n * * * urge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: are you listed in major search engine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: important information thu , 30 jun 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: = ? utf - 8 ? q ? bask your life with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: \" bidstogo \" is places to go , things...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label\n",
       "0  Subject: stock promo mover : cwtd\\n * * * urge...      1\n",
       "1  Subject: are you listed in major search engine...      1\n",
       "2  Subject: important information thu , 30 jun 20...      1\n",
       "3  Subject: = ? utf - 8 ? q ? bask your life with...      1\n",
       "4  Subject: \" bidstogo \" is places to go , things...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tywDHEm4Lmcr",
    "outputId": "5ccc5636-74d4-4fae-dc4f-5e75c1087733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body     0\n",
       "Label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAvIJqv2MsF-",
    "outputId": "66e1288a-a195-4a35-c840-7527862c9132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     Subject: stock promo mover : cwtd\\n * * * urge...      1\n",
      "1     Subject: are you listed in major search engine...      1\n",
      "2     Subject: important information thu , 30 jun 20...      1\n",
      "3     Subject: = ? utf - 8 ? q ? bask your life with...      1\n",
      "4     Subject: \" bidstogo \" is places to go , things...      1\n",
      "...                                                 ...    ...\n",
      "9995  Subject: monday 22 nd oct\\n louise ,\\n do you ...      0\n",
      "9996  Subject: missing bloomberg deals\\n stephanie -...      0\n",
      "9997  Subject: eops salary survey questionnaire\\n we...      0\n",
      "9998  Subject: q 3 comparison\\n hi louise ,\\n i have...      0\n",
      "9999  Subject: confidential folder to safely pass in...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate values based on 'Body' and 'Label' columns\n",
    "df = df.drop_duplicates(subset=['Body', 'Label'])\n",
    "\n",
    "# Display the DataFrame after removing duplicates\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWG0RxlEN6Xq",
    "outputId": "0ea96bca-6b1a-4c0b-a6f4-6fe7380c83f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     subject: stock promo mover : cwtd\\n * * * urge...      1\n",
      "1     subject: are you listed in major search engine...      1\n",
      "2     subject: important information thu , 30 jun 20...      1\n",
      "3     subject: = ? utf - 8 ? q ? bask your life with...      1\n",
      "4     subject: \" bidstogo \" is places to go , things...      1\n",
      "...                                                 ...    ...\n",
      "9995  subject: monday 22 nd oct\\n louise ,\\n do you ...      0\n",
      "9996  subject: missing bloomberg deals\\n stephanie -...      0\n",
      "9997  subject: eops salary survey questionnaire\\n we...      0\n",
      "9998  subject: q 3 comparison\\n hi louise ,\\n i have...      0\n",
      "9999  subject: confidential folder to safely pass in...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Lowercase\n",
    "\n",
    "# Convert the 'Body' column to lowercase\n",
    "df['Body'] = df['Body'].str.lower()\n",
    "\n",
    "# Display the DataFrame with lowercase 'Body' column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmKL85yMOiJk",
    "outputId": "4eb3c084-3e00-4f42-fa58-601aca00e1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     subject stock promo mover  cwtd\\n    urgent in...      1\n",
      "1     subject are you listed in major search engines...      1\n",
      "2     subject important information thu  30 jun 2005...      1\n",
      "3     subject   utf  8  q  bask your life with  \\n  ...      1\n",
      "4     subject  bidstogo  is places to go  things to ...      1\n",
      "...                                                 ...    ...\n",
      "9995  subject monday 22 nd oct\\n louise \\n do you ha...      0\n",
      "9996  subject missing bloomberg deals\\n stephanie \\n...      0\n",
      "9997  subject eops salary survey questionnaire\\n we ...      0\n",
      "9998  subject q 3 comparison\\n hi louise \\n i have a...      0\n",
      "9999  subject confidential folder to safely pass inf...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Removing the punctuation form the body column\n",
    "\n",
    "import re\n",
    "\n",
    "# Convert 'Body' column to string\n",
    "df['Body'] = df['Body'].astype(str)\n",
    "\n",
    "# Remove punctuation from the 'Body' column\n",
    "df['Body'] = df['Body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Display the DataFrame with removed punctuation\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtaJ6jXKW3rb",
    "outputId": "8d2ccd25-1c12-463e-e0a5-b5e5018ad403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohammedpathariya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87p1Gg3fQA3b",
    "outputId": "fde6829c-8dd3-4b96-cc1e-f766cc45d6ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     subject stock promo mover cwtd urgent investor...      1\n",
      "1     subject listed major search engines submitting...      1\n",
      "2     subject important information thu 30 jun 2005 ...      1\n",
      "3     subject utf 8 q bask life utf 8 q individual i...      1\n",
      "4     subject bidstogo places go things hello privac...      1\n",
      "...                                                 ...    ...\n",
      "9995  subject monday 22 nd oct louise half hour free...      0\n",
      "9996  subject missing bloomberg deals stephanie beli...      0\n",
      "9997  subject eops salary survey questionnaire need ...      0\n",
      "9998  subject q 3 comparison hi louise comparison fi...      0\n",
      "9999  subject confidential folder safely pass inform...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords from the 'Body' column\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Body'] = df['Body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Display the DataFrame with removed stopwords\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-9p45lRW9Le",
    "outputId": "7c280051-97ad-421b-9fc3-dee9c88a9dfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohammedpathariya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading punkt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hahkRBQQaF0",
    "outputId": "ee5fdbfc-2023-4fc9-ab0e-7e24e60ab23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     [subject, stock, promo, mover, cwtd, urgent, i...      1\n",
      "1     [subject, listed, major, search, engines, subm...      1\n",
      "2     [subject, important, information, thu, 30, jun...      1\n",
      "3     [subject, utf, 8, q, bask, life, utf, 8, q, in...      1\n",
      "4     [subject, bidstogo, places, go, things, hello,...      1\n",
      "...                                                 ...    ...\n",
      "9995  [subject, monday, 22, nd, oct, louise, half, h...      0\n",
      "9996  [subject, missing, bloomberg, deals, stephanie...      0\n",
      "9997  [subject, eops, salary, survey, questionnaire,...      0\n",
      "9998  [subject, q, 3, comparison, hi, louise, compar...      0\n",
      "9999  [subject, confidential, folder, safely, pass, ...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Tokenize the 'Body' column\n",
    "df['Body'] = df['Body'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Display the DataFrame with tokenized 'Body' column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbwfAhc8RO22",
    "outputId": "0d70915a-dc3b-4b05-fc45-6a422cd93483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Body  Label\n",
      "0     [subject, stock, promo, mover, cwtd, urgent, i...      1\n",
      "1     [subject, list, major, search, engin, submit, ...      1\n",
      "2     [subject, import, inform, thu, 30, jun, 2005, ...      1\n",
      "3     [subject, utf, 8, q, bask, life, utf, 8, q, in...      1\n",
      "4     [subject, bidstogo, place, go, thing, hello, p...      1\n",
      "...                                                 ...    ...\n",
      "9995  [subject, monday, 22, nd, oct, louis, half, ho...      0\n",
      "9996  [subject, miss, bloomberg, deal, stephani, bel...      0\n",
      "9997  [subject, eop, salari, survey, questionnair, n...      0\n",
      "9998  [subject, q, 3, comparison, hi, louis, compari...      0\n",
      "9999  [subject, confidenti, folder, safe, pass, info...      0\n",
      "\n",
      "[9687 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # Perform stemming on each token\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Apply the stemming function to the 'Body' column\n",
    "df['Body'] = df['Body'].apply(stem_tokens)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and labels (y)\n",
    "x = df['Body']\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qlmYM9B8X-dm"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dCcWmghOYA53"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hygGsRjtYH02"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "3WzFf_Q0YNHl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 100  # define the maximum length for padding\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eog-909eYa6q",
    "outputId": "ef1f4c8a-62d0-4d7b-decf-e8670d3faf7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "243/243 [==============================] - 10s 37ms/step - loss: 0.6113 - accuracy: 0.6530\n",
      "Epoch 2/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.5583 - accuracy: 0.7047\n",
      "Epoch 3/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.5274 - accuracy: 0.7274\n",
      "Epoch 4/100\n",
      "243/243 [==============================] - 10s 39ms/step - loss: 0.5105 - accuracy: 0.7374\n",
      "Epoch 5/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.5007 - accuracy: 0.7446\n",
      "Epoch 6/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.4934 - accuracy: 0.7505\n",
      "Epoch 7/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.4647 - accuracy: 0.7695\n",
      "Epoch 8/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.4442 - accuracy: 0.7801\n",
      "Epoch 9/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.4355 - accuracy: 0.7872\n",
      "Epoch 10/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.4309 - accuracy: 0.7866\n",
      "Epoch 11/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.4230 - accuracy: 0.7949\n",
      "Epoch 12/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.4058 - accuracy: 0.8050\n",
      "Epoch 13/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3985 - accuracy: 0.8113\n",
      "Epoch 14/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3904 - accuracy: 0.8140\n",
      "Epoch 15/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3807 - accuracy: 0.8201\n",
      "Epoch 16/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3795 - accuracy: 0.8177\n",
      "Epoch 17/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3707 - accuracy: 0.8271\n",
      "Epoch 18/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.3621 - accuracy: 0.8281\n",
      "Epoch 19/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3552 - accuracy: 0.8317\n",
      "Epoch 20/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3488 - accuracy: 0.8364\n",
      "Epoch 21/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3485 - accuracy: 0.8362\n",
      "Epoch 22/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3454 - accuracy: 0.8384\n",
      "Epoch 23/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3338 - accuracy: 0.8441\n",
      "Epoch 24/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.3376 - accuracy: 0.8441\n",
      "Epoch 25/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3262 - accuracy: 0.8480\n",
      "Epoch 26/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.3280 - accuracy: 0.8435\n",
      "Epoch 27/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3239 - accuracy: 0.8522\n",
      "Epoch 28/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3181 - accuracy: 0.8526\n",
      "Epoch 29/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3103 - accuracy: 0.8582\n",
      "Epoch 30/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3104 - accuracy: 0.8564\n",
      "Epoch 31/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.3050 - accuracy: 0.8570\n",
      "Epoch 32/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.3141 - accuracy: 0.8583\n",
      "Epoch 33/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2993 - accuracy: 0.8627\n",
      "Epoch 34/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2976 - accuracy: 0.8657\n",
      "Epoch 35/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2921 - accuracy: 0.8682\n",
      "Epoch 36/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2914 - accuracy: 0.8662\n",
      "Epoch 37/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2872 - accuracy: 0.8672\n",
      "Epoch 38/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.2860 - accuracy: 0.8694\n",
      "Epoch 39/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2786 - accuracy: 0.8704\n",
      "Epoch 40/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2821 - accuracy: 0.8707\n",
      "Epoch 41/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2766 - accuracy: 0.8742\n",
      "Epoch 42/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2739 - accuracy: 0.8756\n",
      "Epoch 43/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.2662 - accuracy: 0.8836\n",
      "Epoch 44/100\n",
      "243/243 [==============================] - 10s 41ms/step - loss: 0.2730 - accuracy: 0.8755\n",
      "Epoch 45/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2597 - accuracy: 0.8842\n",
      "Epoch 46/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2588 - accuracy: 0.8836\n",
      "Epoch 47/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2549 - accuracy: 0.8855\n",
      "Epoch 48/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2503 - accuracy: 0.8908\n",
      "Epoch 49/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2501 - accuracy: 0.8904\n",
      "Epoch 50/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2464 - accuracy: 0.8915\n",
      "Epoch 51/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.2430 - accuracy: 0.8911\n",
      "Epoch 52/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2381 - accuracy: 0.8957\n",
      "Epoch 53/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2398 - accuracy: 0.8924\n",
      "Epoch 54/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2355 - accuracy: 0.8970\n",
      "Epoch 55/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.2336 - accuracy: 0.8987\n",
      "Epoch 56/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2298 - accuracy: 0.9000\n",
      "Epoch 57/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2282 - accuracy: 0.8979\n",
      "Epoch 58/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.2337 - accuracy: 0.8957\n",
      "Epoch 59/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2201 - accuracy: 0.9040\n",
      "Epoch 60/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2180 - accuracy: 0.9037\n",
      "Epoch 61/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2172 - accuracy: 0.9070\n",
      "Epoch 62/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2126 - accuracy: 0.9098\n",
      "Epoch 63/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2071 - accuracy: 0.9117\n",
      "Epoch 64/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.2101 - accuracy: 0.9102\n",
      "Epoch 65/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2063 - accuracy: 0.9129\n",
      "Epoch 66/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.2056 - accuracy: 0.9097\n",
      "Epoch 67/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2073 - accuracy: 0.9133\n",
      "Epoch 68/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.2007 - accuracy: 0.9147\n",
      "Epoch 69/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1958 - accuracy: 0.9134\n",
      "Epoch 70/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1883 - accuracy: 0.9213\n",
      "Epoch 71/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.1950 - accuracy: 0.9174\n",
      "Epoch 72/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1873 - accuracy: 0.9199\n",
      "Epoch 73/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1816 - accuracy: 0.9236\n",
      "Epoch 74/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1928 - accuracy: 0.9186\n",
      "Epoch 75/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1763 - accuracy: 0.9257\n",
      "Epoch 76/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1725 - accuracy: 0.9267\n",
      "Epoch 77/100\n",
      "243/243 [==============================] - 10s 39ms/step - loss: 0.1693 - accuracy: 0.9298\n",
      "Epoch 78/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1822 - accuracy: 0.9249\n",
      "Epoch 79/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1680 - accuracy: 0.9276\n",
      "Epoch 80/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1706 - accuracy: 0.9295\n",
      "Epoch 81/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.2415 - accuracy: 0.8953\n",
      "Epoch 82/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1965 - accuracy: 0.9161\n",
      "Epoch 83/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1669 - accuracy: 0.9311\n",
      "Epoch 84/100\n",
      "243/243 [==============================] - 9s 39ms/step - loss: 0.1570 - accuracy: 0.9339\n",
      "Epoch 85/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1599 - accuracy: 0.9324\n",
      "Epoch 86/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.1518 - accuracy: 0.9384\n",
      "Epoch 87/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1483 - accuracy: 0.9404\n",
      "Epoch 88/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1569 - accuracy: 0.9364\n",
      "Epoch 89/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1558 - accuracy: 0.9339\n",
      "Epoch 90/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.1470 - accuracy: 0.9387\n",
      "Epoch 91/100\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.1467 - accuracy: 0.9400\n",
      "Epoch 92/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1437 - accuracy: 0.9433\n",
      "Epoch 93/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.1451 - accuracy: 0.9424\n",
      "Epoch 94/100\n",
      "243/243 [==============================] - 9s 38ms/step - loss: 0.1409 - accuracy: 0.9443\n",
      "Epoch 95/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1440 - accuracy: 0.9421\n",
      "Epoch 96/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1390 - accuracy: 0.9436\n",
      "Epoch 97/100\n",
      "243/243 [==============================] - 9s 39ms/step - loss: 0.1349 - accuracy: 0.9446\n",
      "Epoch 98/100\n",
      "243/243 [==============================] - 9s 37ms/step - loss: 0.1346 - accuracy: 0.9457\n",
      "Epoch 99/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.1257 - accuracy: 0.9495\n",
      "Epoch 100/100\n",
      "243/243 [==============================] - 9s 36ms/step - loss: 0.1301 - accuracy: 0.9471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28d93f490>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Reshape the input data\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "rnnlstm = Sequential()\n",
    "rnnlstm.add(LSTM(100, input_shape=(x_train.shape[1], 1)))\n",
    "rnnlstm.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "rnnlstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "rnnlstm.fit(x_train, y_train, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0GbicvRfN06",
    "outputId": "c45ce7dd-b8a2-4be0-fbbf-ee8d2757f550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 12ms/step - loss: 0.5599 - accuracy: 0.8292\n",
      "Test Loss: 0.5599117279052734\n",
      "Test Accuracy: 0.8292053937911987\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = rnnlstm.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLlc4SWDhSbz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7hKYiVYchQi8"
   },
   "outputs": [],
   "source": [
    "#Defining the preprocessing function\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_paragraph(paragraph):\n",
    "    # Convert to lowercase\n",
    "    paragraph = paragraph.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    paragraph = re.sub(r'[^\\w\\s]', '', paragraph)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    paragraph = ' '.join(word for word in word_tokenize(paragraph) if word not in stop_words)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(paragraph)\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bv26b2ug_r1",
    "outputId": "8f4859af-a942-43fc-eda3-a5e7ebe47df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give some content to predict weather it is Spam or Ham : Holiday for a day due to sickness. I am an employee of your company. I am sick for the past day and I am still not feeling well. I request you to give me a days leave. Thank you\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted label: Ham\n"
     ]
    }
   ],
   "source": [
    " # Take the input from the user directly\n",
    "email_text = input('Give some content to predict weather it is Spam or Ham : ')\n",
    "\n",
    "# Preprocess the input paragraph\n",
    "preprocessed_paragraph = preprocess_paragraph(email_text)\n",
    "\n",
    "# Convert the preprocessed paragraph into a sequence of integers\n",
    "sequence = tokenizer.texts_to_sequences([preprocessed_paragraph])\n",
    "\n",
    "# Pad the sequence to have the same length as the training data\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = rnnlstm.predict(padded_sequence)\n",
    "\n",
    "# Get the predicted class (0 for ham, 1 for spam)\n",
    "predicted_class = predictions.argmax(axis=1)[0]\n",
    "\n",
    "# Map the predicted class to 'spam' or 'ham'\n",
    "label = \"Spam\" if predicted_class == 0 else \"Ham\"\n",
    "\n",
    "# Print the predicted label\n",
    "\n",
    "print(\"Predicted label:\", label)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
